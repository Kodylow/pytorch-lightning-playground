{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import random_split, DataLoader"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the network architecture:\n",
    "\n",
    "This part of the code defines a simple feed-forward neural network using PyTorch's nn.Sequential container. This network will be used to classify MNIST images, which are grayscale images of handwritten digits (0-9).\n",
    "\n",
    "1. nn.Linear(28*28, 64): This is the first layer of the network, a linear (also known as fully connected) layer. It takes as input a flattened MNIST image. Since each image is 28x28 pixels, this results in 784 input features. The output size is 64, meaning this layer consists of 64 neurons.\n",
    "\n",
    "2. nn.ReLU(): This is the activation function for the first layer. It introduces non-linearity into the model, allowing the network to learn more complex patterns. ReLU stands for Rectified Linear Unit and it operates element-wise on the output of the previous layer, effectively clipping negative values to zero.\n",
    "\n",
    "3. nn.Linear(64, 64): This is the second layer of the network, another linear layer. It takes as input the output of the first layer (and its activation function), which has a size of 64. The output size is also 64.\n",
    "\n",
    "4. nn.ReLU(): This is the activation function for the second layer.\n",
    "\n",
    "5. nn.Linear(64, 10): This is the final layer of the network. It takes as input the output of the second layer (and its activation function), which has a size of 64. The output size is 10, corresponding to the 10 possible classes (digits 0-9) of the MNIST dataset.\n",
    "\n",
    "The output of this network will be a 10-dimensional vector for each input image, where each dimension corresponds to the predicted score for each class. The class with the highest score is the model's prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the network\n",
    "model = nn.Sequential(\n",
    "    # MNIST images are 28x28, so 784 input features\n",
    "    # 64 is the number of hidden units\n",
    "    nn.Linear(28*28, 64), # 28*28 = 784\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(64, 64),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.1), # drop 10% of the neuron if we're overfitting\n",
    "    nn.Linear(64, 10) # 10 classes\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define my Optimizer:\n",
    "\n",
    "The optimizer is responsible for updating the model's parameters in the direction that minimizes the loss function. Here, Stochastic Gradient Descent (SGD) is used with a learning rate of 0.01. The model.parameters() method is used to fetch the parameters of the model to be optimized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define my optimizer\n",
    "params = model.parameters()\n",
    "optimizer = optim.SGD(params, lr=0.01)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define my Loss Function\n",
    "\n",
    "The loss function measures the difference between the predicted value (output of the model) and the target value (the true value). Here, the cross entropy loss function is used, which is commonly used to train classification models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define my loss function\n",
    "loss = nn.CrossEntropyLoss()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Train, Test Split and Loaders\n",
    "\n",
    "The MNIST dataset is split into three parts: training set (used to train the model), validation set (used to evaluate model during training) and test set (used to test the model after training). The MNIST training set contains 60,000 examples, and the test set contains 10,000 examples. The validation set contains 10,000 examples from the MNIST training set. The validation set is used to evaluate the model during training by computing and reporting metrics such as accuracy. The MNIST dataset is provided as part of the torchvision package, which downloads and loads the dataset into a PyTorch dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train, val split\n",
    "train_data = datasets.MNIST(root='data', train=True, download=True, transform=transforms.ToTensor())\n",
    "train, val = random_split(train_data, [55000, 5000])\n",
    "train_loader = DataLoader(train, batch_size=32)\n",
    "val_loader = DataLoader(val, batch_size=32)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the 5 Step PyTorch Training Loop\n",
    "\n",
    "The training loop is the heart of the training process. It consists of five steps:\n",
    "\n",
    "1. Forward Pass: The model makes a prediction based on the input data x (flattened images). The output l (logits) is the raw, unnormalized scores for each class.\n",
    "\n",
    "2. Compute the Objective Function: The loss function J is computed using the predicted output l and the true labels y. This measures the difference between the model's predictions and the true values.\n",
    "\n",
    "3. Clean the Gradients: Before computing the gradients, we need to set the existing gradients to zero. This is because PyTorch accumulates gradients, and we don't want to mix up gradients between mini batches.\n",
    "\n",
    "4. Compute the Partial Derivative with respect to the Parameters: The backward() function computes the gradient of the loss function J with respect to the model parameters. This is used in the next step for the gradient descent update.\n",
    "\n",
    "5. Step in the Opposite Direction: The step() function updates the model parameters in the opposite direction of the gradients to minimize the loss function. This is done by the optimizer, which in this case is Stochastic Gradient Descent (SGD).\n",
    "\n",
    "The loop is run for a specified number of epochs, where an epoch is one complete pass through the entire training dataset. In this case, the number of epochs is set to 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, training loss: 0.23\n",
      "Epoch 1, validation loss: 0.25\n",
      "Epoch 2, training loss: 0.21\n",
      "Epoch 2, validation loss: 0.24\n",
      "Epoch 3, training loss: 0.20\n",
      "Epoch 3, validation loss: 0.22\n",
      "Epoch 4, training loss: 0.18\n",
      "Epoch 4, validation loss: 0.21\n",
      "Epoch 5, training loss: 0.17\n",
      "Epoch 5, validation loss: 0.20\n"
     ]
    }
   ],
   "source": [
    "# Create the training and validation loops\n",
    "epochs = 5\n",
    "for epoch in range(epochs):\n",
    "    losses = []\n",
    "    for batch in train_loader:\n",
    "        x, y = batch\n",
    "\n",
    "        # Flatten the image\n",
    "        # x: b x 1 x 28 x 28 -> b x 784\n",
    "        b = x.size(0)\n",
    "        x = x.view(b, -1) # -1 means infer this dimension\n",
    "\n",
    "        # 1. Forward pass\n",
    "        l = model(x) # l: logits\n",
    "\n",
    "        # 2. Compute the objective function\n",
    "        J = loss(l, y)\n",
    "\n",
    "        # 3. Cleaning the gradients\n",
    "        model.zero_grad()\n",
    "        # optimizer.zero_grad()\n",
    "        # params.grad.zero_()\n",
    "\n",
    "        # 4. Compute the partial derivatives of J w.r.t parameters\n",
    "        J.backward()\n",
    "        # params.grad.add_(dJ/dparams)\n",
    "\n",
    "        # 5. Step in the opposite direction of the gradient\n",
    "        optimizer.step()\n",
    "        # with torch.no_grad(): params = params - lr * params.grad\n",
    "\n",
    "        losses.append(J.item())\n",
    "\n",
    "    print(f'Epoch {epoch + 1}, training loss: {torch.tensor(losses).mean():.2f}')\n",
    "\n",
    "    # Evaluate the model on the validation set\n",
    "    losses = []\n",
    "    for batch in val_loader:\n",
    "        x, y = batch\n",
    "\n",
    "        # x: b x 1 x 28 x 28 -> b x 784\n",
    "        b = x.size(0)\n",
    "        x = x.view(b, -1)\n",
    "\n",
    "        # 1. Forward pass\n",
    "        with torch.no_grad():\n",
    "            l = model(x) # l: logits\n",
    "\n",
    "        # 2. Compute the objective function\n",
    "        J = loss(l, y)\n",
    "\n",
    "        losses.append(J.item())\n",
    "\n",
    "    print(f'Epoch {epoch + 1}, validation loss: {torch.tensor(losses).mean():.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
